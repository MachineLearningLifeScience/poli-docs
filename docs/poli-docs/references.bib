---
---

@article{stanton2022accelerating,
  title = {Accelerating Bayesian Optimization for Biological Sequence Design
           with Denoising Autoencoders},
  author = {Stanton, Samuel and Maddox, Wesley and Gruver, Nate and Maffettone,
            Phillip and Delaney, Emily and Greenside, Peyton and Wilson, Andrew
            Gordon},
  journal = {arXiv preprint arXiv:2203.12742},
  year = {2022},
}

@article{ShrakeRupley:SASA:1973,
  title = {Environment and exposure to solvent of protein atoms. Lysozyme and
           insulin},
  volume = {79},
  issn = {00222836},
  doi = {10.1016/0022-2836(73)90011-9},
  number = {2},
  journal = {Journal of Molecular Biology},
  author = {Shrake, A. and Rupley, J.A.},
  year = {1973},
  month = {Sep},
  pages = {351–371},
  language = {en},
}


@article{Brown:guacamol:2019,
  title = {GuacaMol: Benchmarking Models for de Novo Molecular Design},
  volume = {59},
  ISSN = {1549-9596, 1549-960X},
  DOI = {10.1021/acs.jcim.8b00839},
  number = {3},
  journal = {Journal of Chemical Information and Modeling},
  author = {Brown, Nathan and Fiscato, Marco and Segler, Marwin H.S. and Vaucher
            , Alain C.},
  year = {2019},
  month = mar,
  pages = {1096–1108},
  language = {en},
}


@article{Huang:TDC:2021,
  title = {Therapeutics Data Commons: Machine Learning Datasets and Tasks for
           Drug Discovery and Development},
  author = {Huang, Kexin and Fu, Tianfan and Gao, Wenhao and Zhao, Yue and
            Roohani, Yusuf and Leskovec, Jure and Coley, Connor W and Xiao, Cao
            and Sun, Jimeng and Zitnik, Marinka},
  journal = {Proceedings of Neural Information Processing Systems, NeurIPS
             Datasets and Benchmarks},
  year = {2021},
}

@article{Olivecrona:DeNovoRL:2017,
  title = {Molecular de-novo design through deep reinforcement learning},
  volume = {9},
  ISSN = {1758-2946},
  DOI = {10.1186/s13321-017-0235-x},
  number = {1},
  journal = {Journal of Cheminformatics},
  author = {Olivecrona, Marcus and Blaschke, Thomas and Engkvist, Ola and Chen,
            Hongming},
  year = {2017},
  month = sep,
  pages = {48},
  language = {en},
}

 @article{Blaabjerg:RaSP:2023,
  title = {Rapid protein stability prediction using deep learning
           representations},
  volume = {12},
  issn = {2050-084X},
  doi = {10.7554/eLife.82593},
  abstractnote = {Predicting the thermodynamic stability of proteins is a common
                  and widely used step in protein engineering, and when
                  elucidating the molecular mechanisms behind evolution and
                  disease. Here, we present RaSP, a method for making rapid and
                  accurate predictions of changes in protein stability by
                  leveraging deep learning representations. RaSP performs on-par
                  with biophysics-based methods and enables saturation
                  mutagenesis stability predictions in less than a second per
                  residue. We use RaSP to calculate ∼ 230 million stability
                  changes for nearly all single amino acid changes in the human
                  proteome, and examine variants observed in the human
                  population. We find that variants that are common in the
                  population are substantially depleted for severe
                  destabilization, and that there are substantial differences
                  between benign and pathogenic variants, highlighting the role
                  of protein stability in genetic diseases. RaSP is freely
                  available—including via a Web interface—and enables large-scale
                  analyses of stability in experimental and predicted protein
                  structures.},
  journal = {eLife},
  publisher = {eLife Sciences Publications, Ltd},
  author = {Blaabjerg, Lasse M and Kassem, Maher M and Good, Lydia L and Jonsson
            , Nicolas and Cagiada, Matteo and Johansson, Kristoffer E and Boomsma
            , Wouter and Stein, Amelie and Lindorff-Larsen, Kresten},
  editor = {Faraldo-Gómez, José D and Weigel, Detlef and Ben-Tal, Nir and Echave
            , Julian},
  year = {2023},
  month = may,
  pages = {e82593},
}

@inproceedings{Boomsma:spherical_conv:2017,
  title = {Spherical convolutions and their application in molecular modelling},
  volume = {30},
  url = {
         https://proceedings.neurips.cc/paper_files/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf
         },
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  author = {Boomsma, Wouter and Frellsen, Jes},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and
            Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
}

 @article{GarciaOrtegon:dockstring:2022,
  title = {DOCKSTRING: Easy Molecular Docking Yields Better Benchmarks for
           Ligand Design},
  volume = {62},
  issn = {1549-9596, 1549-960X},
  doi = {10.1021/acs.jcim.1c01334},
  number = {15},
  journal = {Journal of Chemical Information and Modeling},
  author = {García-Ortegón, Miguel and Simm, Gregor N. C. and Tripp, Austin J.
            and Hernández-Lobato, José Miguel and Bender, Andreas and Bacallado,
            Sergio},
  year = {2022},
  month = aug,
  pages = {3486–3502},
  language = {en},
}

@misc{Al-Roomi:continuous_objective_benchmarks:2015,
  author = {Ali R. Al-Roomi},
  title = {{Unconstrained Single-Objective Benchmark Functions Repository}},
  year = {2015},
  address = {Halifax, Nova Scotia, Canada},
  institution = {Dalhousie University, Electrical and Computer Engineering},
  url = {https://www.al-roomi.org/benchmarks/unconstrained},
}

@misc{SurjanovicBingham:test_functions:2013,
  author = {Surjanovic, S. and Bingham, D.},
  title = {Optimization Test Functions and Datasets},
  year = {2013},
  howpublished = {\url{https://www.sfu.ca/~ssurjano/optimization.html}},
  note = {Accessed: 2024-04-12},
}


@inproceedings{Gao:PMO:2022,
  title = {Sample Efficiency Matters: A Benchmark for Practical Molecular
           Optimization},
  author = {Wenhao Gao and Tianfan Fu and Jimeng Sun and Connor W. Coley},
  booktitle = {Thirty-sixth Conference on Neural Information Processing Systems
               Datasets and Benchmarks Track},
  year = {2022},
  url = {https://openreview.net/forum?id=yCZRdI0Y7G},
}

 @article{Deb:NSGA-2:2002,
  title = {A fast and elitist multiobjective genetic algorithm: NSGA-II},
  volume = {6},
  ISSN = {1089778X},
  DOI = {10.1109/4235.996017},
  number = {2},
  journal = {IEEE Transactions on Evolutionary Computation},
  author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  year = {2002},
  month = apr,
  pages = {182–197},
  language = {en},
}

@article{pymoo,
  author = {J. {Blank} and K. {Deb}},
  journal = {IEEE Access},
  title = {pymoo: Multi-Objective Optimization in Python},
  year = {2020},
  volume = {8},
  number = {},
  pages = {89497-89509},
}

 @article{Shahriari:BOReview:2016,
  title = {Taking the Human Out of the Loop: A Review of Bayesian Optimization},
  volume = {104},
  ISSN = {0018-9219, 1558-2256},
  DOI = {10.1109/JPROC.2015.2494218},
  abstractNote = {Big data applications are typically associated with systems
                  involving large numbers of users, massive complex software
                  systems, and large-scale heterogeneous computing and storage
                  architectures. The construction of such systems involves many
                  distributed design choices. The end products (e.g.,
                  recommendation systems, medical analysis tools, real-time game
                  engines, speech recognizers) thus involves many tunable
                  conﬁguration parameters. These parameters are often speciﬁed
                  and hard-coded into the software by various developers or
                  teams. If optimized jointly, these parameters can result in
                  signiﬁcant improvements. Bayesian optimization is a powerful
                  tool for the joint optimization of design choices that is
                  gaining great popularity in recent years. It promises greater
                  automation so as to increase both product quality and human
                  productivity. This review paper introduces Bayesian
                  optimization, highlights some of its methodological aspects,
                  and showcases a wide range of applications.},
  number = {1},
  journal = {Proceedings of the IEEE},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan
            P. and De Freitas, Nando},
  year = {2016},
  month = jan,
  pages = {148–175},
  language = {en},
}

 @inproceedings{Kirschner:LineBO:2019,
  title = {Adaptive and Safe Bayesian Optimization in High Dimensions via
           One-Dimensional Subspaces},
  ISSN = {2640-3498},
  url = {https://proceedings.mlr.press/v97/kirschner19a.html},
  abstractNote = {Bayesian optimization is known to be difficult to scale to
                  high dimensions, because the acquisition step requires solving
                  a non-convex optimization problem in the same search space. In
                  order to scale the method and keep its benefits, we propose an
                  algorithm (LineBO) that restricts the problem to a sequence of
                  iteratively chosen one-dimensional sub-problems that can be
                  solved efficiently. We show that our algorithm converges
                  globally and obtains a fast local rate when the function is
                  strongly convex. Further, if the objective has an invariant
                  subspace, our method automatically adapts to the effective
                  dimension without changing the algorithm. When combined with
                  the SafeOpt algorithm to solve the sub-problems, we obtain the
                  first safe Bayesian optimization algorithm with theoretical
                  guarantees applicable in high-dimensional settings. We evaluate
                  our method on multiple synthetic benchmarks, where we obtain
                  competitive performance. Further, we deploy our algorithm to
                  optimize the beam intensity of the Swiss Free Electron Laser
                  with up to 40 parameters while satisfying safe operation
                  constraints.},
  booktitle = {Proceedings of the 36th International Conference on Machine
               Learning},
  publisher = {PMLR},
  author = {Kirschner, Johannes and Mutny, Mojmir and Hiller, Nicole and
            Ischebeck, Rasmus and Krause, Andreas},
  year = {2019},
  month = may,
  pages = {3429–3438},
  language = {en},
}


 @article{Balandat:botorch:2020,
  title = {BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization},
  url = {http://arxiv.org/abs/1910.06403},
  abstractNote = {Bayesian optimization provides sample-efficient global
                  optimization for a broad range of applications, including
                  automatic machine learning, engineering, physics, and
                  experimental design. We introduce BoTorch, a modern programming
                  framework for Bayesian optimization that combines Monte-Carlo
                  (MC) acquisition functions, a novel sample average
                  approximation optimization approach, auto-differentiation, and
                  variance reduction techniques. BoTorch’s modular design
                  facilitates flexible specification and optimization of
                  probabilistic models written in PyTorch, simplifying
                  implementation of new acquisition functions. Our approach is
                  backed by novel theoretical convergence results and made
                  practical by a distinctive algorithmic foundation that
                  leverages fast predictive distributions, hardware acceleration,
                  and deterministic optimization. We also propose a novel
                  “one-shot” formulation of the Knowledge Gradient, enabled by a
                  combination of our theoretical and software contributions. In
                  experiments, we demonstrate the improved sample efficiency of
                  BoTorch relative to other popular libraries.},
  note = {arXiv:1910.06403 [cs, math, stat]},
  number = {arXiv:1910.06403},
  publisher = {arXiv},
  journal = {arXiv},
  author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and
            Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and
            Bakshy, Eytan},
  year = {2020},
  month = dec,
}

@inproceedings{gardner:gpytorch:2018,
  title = {GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU
           Acceleration},
  author = {Gardner, Jacob R and Pleiss, Geoff and Bindel, David and Weinberger,
            Kilian Q and Wilson, Andrew Gordon},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2018},
}

 @article{GomezBombarelli:VAEsAndOpt:2018,
  title = {Automatic Chemical Design Using a Data-Driven Continuous
           Representation of Molecules},
  volume = {4},
  ISSN = {2374-7943},
  DOI = {10.1021/acscentsci.7b00572},
  abstractNote = {We report a method to convert discrete representations of
                  molecules to and from a multidimensional continuous
                  representation. This model allows us to generate new molecules
                  for efficient exploration and optimization through open-ended
                  spaces of chemical compounds. A deep neural network was trained
                  on hundreds of thousands of existing chemical structures to
                  construct three coupled functions: an encoder, a decoder, and a
                  predictor. The encoder converts the discrete representation of
                  a molecule into a real-valued continuous vector, and the
                  decoder converts these continuous vectors back to discrete
                  molecular representations. The predictor estimates chemical
                  properties from the latent continuous vector representation of
                  the molecule. Continuous representations of molecules allow us
                  to automatically generate novel chemical structures by
                  performing simple operations in the latent space, such as
                  decoding random vectors, perturbing known chemical structures,
                  or interpolating between molecules. Continuous representations
                  also allow the use of powerful gradient-based optimization to
                  efficiently guide the search for optimized functional
                  compounds. We demonstrate our method in the domain of drug-like
                  molecules and also in a set of molecules with fewer that nine
                  heavy atoms.},
  number = {2},
  journal = {ACS Central Science},
  publisher = {American Chemical Society},
  author = {Gómez-Bombarelli, Rafael and Wei, Jennifer N. and Duvenaud, David
            and Hernández-Lobato, José Miguel and Sánchez-Lengeling, Benjamín and
            Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy
            D. and Adams, Ryan P. and Aspuru-Guzik, Alán},
  year = {2018},
  month = feb,
  pages = {268–276},
}

@inproceedings{Hansen:CMA-ES:1996,
  author = {Hansen, N. and Ostermeier, A.},
  booktitle = {Proceedings of IEEE International Conference on Evolutionary
               Computation},
  title = {Adapting arbitrary normal mutation distributions in evolution
           strategies: the covariance matrix adaptation},
  year = {1996},
  volume = {},
  number = {},
  pages = {312-317},
  doi = {10.1109/ICEC.1996.542381},
}


@inproceedings{jain:multiobjective-gflownet:2023,
  title = {Multi-Objective {GF}low{N}ets},
  author = {Jain, Moksh and Raparthy, Sharath Chandra and Hern\'{a}ndez-Garc\'{
            \i}a, Alex and Rector-Brooks, Jarrid and Bengio, Yoshua and Miret,
            Santiago and Bengio, Emmanuel},
  booktitle = {Proceedings of the 40th International Conference on Machine
               Learning},
  pages = {14631--14653},
  year = {2023},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and
            Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = {202},
  series = {Proceedings of Machine Learning Research},
  month = {23--29 Jul},
  publisher = {PMLR},
  pdf = {https://proceedings.mlr.press/v202/jain23a/jain23a.pdf},
  url = {https://proceedings.mlr.press/v202/jain23a.html},
  abstract = {We study the problem of generating <em>diverse</em> candidates in
              the context of Multi-Objective Optimization. In many applications
              of machine learning such as drug discovery and material design, the
              goal is to generate candidates which simultaneously optimize a set
              of potentially conflicting objectives. Moreover, these objectives
              are often imperfect evaluations of some underlying property of
              interest, making it important to generate diverse candidates to
              have multiple options for expensive downstream evaluations. We
              propose Multi-Objective GFlowNets (MOGFNs), a novel method for
              generating diverse Pareto optimal solutions, based on GFlowNets. We
              introduce two variants of MOGFNs: MOGFN-PC, which models a family
              of independent sub-problems defined by a scalarization function,
              with reward-conditional GFlowNets, and MOGFN-AL, which solves a
              sequence of sub-problems defined by an acquisition function in an
              active learning loop. Our experiments on wide variety of synthetic
              and benchmark tasks demonstrate advantages of the proposed methods
              in terms of the Pareto performance and importantly, improved
              candidate diversity, which is the main contribution of this work.},
}

@inproceedings{MarioAICompetition:Baumgarten:2010,
  author = {Togelius, Julian and Karakovskiy, Sergey and Baumgarten, Robin},
  booktitle = {IEEE Congress on Evolutionary Computation},
  title = {The 2009 Mario AI Competition},
  year = {2010},
  volume = {},
  number = {},
  pages = {1-8},
  doi = {10.1109/CEC.2010.5586133},
}

@article{VGLC,
  Author = {Adam James Summerville and Sam Snodgrass and Michael Mateas and
            Santiago Onta~{n}'{o}n Villar},
  Title = {The VGLC: The Video Game Level Corpus},
  Year = {2016},
  Journal = {Proceedings of the 7th Workshop on Procedural Content Generation},
}

@article{Volz:MarioGAN:2018,
  title = {Evolving Mario levels in the latent space of a deep convolutional
           generative adversarial network},
  ISSN = {9781450356183},
  DOI = {10.1145/3205455.3205517},
  abstractNote = {Generative Adversarial Networks (GANs) are a machine learning
                  approach capable of generating novel example outputs across a
                  space of provided training examples. Procedural Content
                  Generation (PCG) of levels for video games could benefit from
                  such models, especially for games where there is a pre-existing
                  corpus of levels to emulate. This paper trains a GAN to
                  generate levels for Super Mario Bros using a level from the
                  Video Game Level Corpus. The approach successfully generates a
                  variety of levels similar to one in the original corpus, but is
                  further improved by application of the Covariance Matrix
                  Adaptation Evolution Strategy (CMA-ES). Specifically, various
                  fitness functions are used to discover levels within the latent
                  space of the GAN that maximize desired properties. Simple
                  static properties are optimized, such as a given distribution
                  of tile types. Additionally, the champion A* agent from the
                  2009 Mario AI competition is used to assess whether a level is
                  playable, and how many jumping actions are required to beat it.
                  These fitness functions allow for the discovery of levels that
                  exist within the space of examples designed by experts, and
                  also guide the search towards levels that fulfill one or more
                  specified objectives.},
  journal = {GECCO 2018 - Proceedings of the 2018 Genetic and Evolutionary
             Computation Conference},
  author = {Volz, Vanessa and Lucas, Simon M. and Schrum, Jacob and Smith, Adam
            and Liu, Jialin and Risi, Sebastian},
  year = {2018},
  pages = {221–228},
}

@misc{Khalifa:marioAIFramework:2009,
  title = {The Mario AI Framework},
  author = {Ahmed Khalifa},
  year = {2009},
  howpublished = {\url{https://github.com/amidos2006/Mario-AI-Framework}},
  note = {Accessed: 20/03/2024},
}

@article{Li:MOConditional:2018,
  title = {Multi-objective de novo drug design with conditional graph generative
           model},
  volume = {10},
  ISSN = {1758-2946},
  DOI = {10.1186/s13321-018-0287-6},
  number = {1},
  journal = {Journal of Cheminformatics},
  author = {Li, Yibo and Zhang, Liangren and Liu, Zhenming},
  year = {2018},
  month = jul,
  pages = {33},
  language = {en},
}

@article{Jin:MOInterpretable:2020,
  title = {Multi-Objective Molecule Generation using Interpretable Substructures
           },
  volume = {119},
  journal = {Proceedings of the 37 th International Conference on Machine
             Learning PMLR},
  author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  year = {2020},
  language = {en},
}


@article{Ertl:SA:2009,
  title = {Estimation of synthetic accessibility score of drug-like molecules
           based on molecular complexity and fragment contributions},
  volume = {1},
  ISSN = {1758-2946},
  DOI = {10.1186/1758-2946-1-8},
  number = {1},
  journal = {Journal of Cheminformatics},
  author = {Ertl, Peter and Schuffenhauer, Ansgar},
  year = {2009},
  month = jun,
  pages = {8},
  language = {en},
}

@article{Polykovskiy:MOSES:2020,
  title = {Molecular Sets (MOSES): A Benchmarking Platform for Molecular
           Generation Models},
  volume = {11},
  ISSN = {1663-9812},
  url = {https://www.frontiersin.org/articles/10.3389/fphar.2020.565644},
  author = {Polykovskiy, Daniil and Zhebrak, Alexander and Sanchez-Lengeling,
            Benjamin and Golovanov, Sergey and Tatanov, Oktai and Belyaev,
            Stanislav and Kurbanov, Rauf and Artamonov, Aleksey and Aladinskiy,
            Vladimir and Veselov, Mark and Kadurin, Artur and Johansson, Simon
            and Chen, Hongming and Nikolenko, Sergey and Aspuru-Guzik, Alán and
            Zhavoronkov, Alex},
  year = {2020},
}

 @article{Schymkowitz:foldx:2005,
  title = {The FoldX web server: an online force field},
  volume = {33},
  ISSN = {0305-1048},
  DOI = {10.1093/nar/gki387},
  journal = {Nucleic Acids Research},
  author = {Schymkowitz, Joost and Borg, Jesper and Stricher, Francois and Nys,
            Robby and Rousseau, Frederic and Serrano, Luis},
  year = {2005},
  month = jul,
  pages = {W382–W388},
}

@misc{rdkit,
  author = {RDKit},
  title = {RDKit: Open-source cheminformatics},
  year = {2006},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/rdkit/rdkit}},
}

@misc{Hvarfner:VanillaBO:2024,
  title = {Vanilla Bayesian Optimization Performs Great in High Dimensions},
  author = {Carl Hvarfner and Erik Orm Hellsten and Luigi Nardi},
  year = {2024},
  eprint = {2402.02229},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
}


@inproceedings{Eriksson:saasbo:2021,
  title = {High-dimensional {Bayesian} optimization with sparse axis-aligned
           subspaces},
  author = {Eriksson, David and Jankowiak, Martin},
  booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in
               Artificial Intelligence},
  pages = {493--503},
  year = {2021},
  editor = {de Campos, Cassio and Maathuis, Marloes H.},
  volume = {161},
  series = {Proceedings of Machine Learning Research},
  month = {27--30 Jul},
  publisher = {PMLR},
  pdf = {https://proceedings.mlr.press/v161/eriksson21a/eriksson21a.pdf},
  url = {https://proceedings.mlr.press/v161/eriksson21a.html},
  abstract = {Bayesian optimization (BO) is a powerful paradigm for efficient
              optimization of black-box objective functions. High-dimensional BO
              presents a particular challenge, in part because the curse of
              dimensionality makes it difficult to define—as well as do inference
              over—a suitable class of surrogate models. We argue that Gaussian
              process surrogate models defined on sparse axis-aligned subspaces
              offer an attractive compromise between flexibility and parsimony.
              We demonstrate that our approach, which relies on Hamiltonian Monte
              Carlo for inference, can rapidly identify sparse subspaces relevant
              to modeling the unknown objective function, enabling
              sample-efficient high-dimensional BO. In an extensive suite of
              experiments comparing to existing methods for high-dimensional BO
              we demonstrate that our algorithm, Sparse Axis-Aligned Subspace BO
              (SAASBO), achieves excellent performance on several synthetic and
              real-world problems without the need to set problem-specific
              hyperparameters.},
}

@inproceedings{Letham:ALEBO:2020,
     title={Re-Examining Linear Embeddings for High-Dimensional Bayesian Optimization},
     volume={33},
     url={https://proceedings.neurips.cc/paper/2020/hash/10fb6cfa4c990d2bad5ddef4f70e8ba2-Abstract.html},
     booktitle={Advances in Neural Information Processing Systems},
     publisher={Curran Associates, Inc.},
     author={Letham, Ben and Calandra, Roberto and Rai, Akshara and Bakshy, Eytan},
     year={2020},
     pages={1546-1558}
}

@inproceedings{Eriksson:Turbo:2019,
    title={Scalable Global Optimization via Local Bayesian Optimization},
    volume={32},
    url={https://proceedings.neurips.cc/paper/2019/hash/6c990b7aca7bc7058f5e98ea909e924b-Abstract.html},
    booktitle={Advances in Neural Information Processing Systems},
    publisher={Curran Associates, Inc.},
    author={Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
    year={2019}
}

@inproceedings{Papenmeier:BAxUS:2022,
    title={Increasing the Scope as You Learn: Adaptive Bayesian Optimization in Nested Subspaces},
    url={https://openreview.net/forum?id=e4Wf6112DI},
    author={Papenmeier, Leonard and Nardi, Luigi and Poloczek, Matthias},
    year={2022},
    month=may,
    language={en}
}

@misc{Papenmeier:Bounce:2024,
      title={Bounce: Reliable High-Dimensional Bayesian Optimization for Combinatorial and Mixed Spaces}, 
      author={Leonard Papenmeier and Luigi Nardi and Matthias Poloczek},
      year={2024},
      eprint={2307.00618},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Daulton:ProbRep:2022,
      title={Bayesian Optimization over Discrete and Mixed Spaces via Probabilistic Reparameterization}, 
      author={Samuel Daulton and Xingchen Wan and and David Eriksson and Maximilian Balandat and Michael A. Osborne and Eytan Bakshy},
      booktitle={Advances in Neural Information Processing Systems 35},
      year={2022}
}

@misc{Stanton:Ehrlich:2024,
      title={Closed-Form Test Functions for Biophysical Sequence Optimization Algorithms}, 
      author={Samuel Stanton and Robert Alberstein and Nathan Frey and Andrew Watkins and Kyunghyun Cho},
      year={2024},
      eprint={2407.00236},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.00236}, 
}
