---
---

@article{stanton2022accelerating,
  title   = {Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders},
  author  = {Stanton, Samuel and Maddox, Wesley and Gruver, Nate and Maffettone, Phillip and Delaney, Emily and Greenside, Peyton and Wilson, Andrew Gordon},
  journal = {arXiv preprint arXiv:2203.12742},
  year    = {2022}
}

@article{ShrakeRupley:SASA:1973,
  title    = {Environment and exposure to solvent of protein atoms. Lysozyme and insulin},
  volume   = {79},
  issn     = {00222836},
  doi      = {10.1016/0022-2836(73)90011-9},
  number   = {2},
  journal  = {Journal of Molecular Biology},
  author   = {Shrake, A. and Rupley, J.A.},
  year     = {1973},
  month    = {Sep},
  pages    = {351–371},
  language = {en}
}

@inproceedings{huang:TDC:2021,
  title     = {Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development},
  author    = {Kexin Huang and Tianfan Fu and Wenhao Gao and Yue Zhao and Yusuf H Roohani and Jure Leskovec and Connor W. Coley and Cao Xiao and Jimeng Sun and Marinka Zitnik},
  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
  year      = {2021},
  url       = {https://openreview.net/forum?id=8nvgnORnoWr}
}

 @article{Blaabjerg:RaSP:2023,
  title        = {Rapid protein stability prediction using deep learning representations},
  volume       = {12},
  issn         = {2050-084X},
  doi          = {10.7554/eLife.82593},
  abstractnote = {Predicting the thermodynamic stability of proteins is a common and widely used step in protein engineering, and when elucidating the molecular mechanisms behind evolution and disease. Here, we present RaSP, a method for making rapid and accurate predictions of changes in protein stability by leveraging deep learning representations. RaSP performs on-par with biophysics-based methods and enables saturation mutagenesis stability predictions in less than a second per residue. We use RaSP to calculate ∼ 230 million stability changes for nearly all single amino acid changes in the human proteome, and examine variants observed in the human population. We find that variants that are common in the population are substantially depleted for severe destabilization, and that there are substantial differences between benign and pathogenic variants, highlighting the role of protein stability in genetic diseases. RaSP is freely available—including via a Web interface—and enables large-scale analyses of stability in experimental and predicted protein structures.},
  journal      = {eLife},
  publisher    = {eLife Sciences Publications, Ltd},
  author       = {Blaabjerg, Lasse M and Kassem, Maher M and Good, Lydia L and Jonsson, Nicolas and Cagiada, Matteo and Johansson, Kristoffer E and Boomsma, Wouter and Stein, Amelie and Lindorff-Larsen, Kresten},
  editor       = {Faraldo-Gómez, José D and Weigel, Detlef and Ben-Tal, Nir and Echave, Julian},
  year         = {2023},
  month        = may,
  pages        = {e82593}
}

@inproceedings{Boomsma:spherical_conv:2017,
  title     = {Spherical convolutions and their application in molecular modelling},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  author    = {Boomsma, Wouter and Frellsen, Jes},
  editor    = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year      = {2017}
}

 @article{GarciaOrtegon:dockstring:2022,
  title    = {DOCKSTRING: Easy Molecular Docking Yields Better Benchmarks for Ligand Design},
  volume   = {62},
  issn     = {1549-9596, 1549-960X},
  doi      = {10.1021/acs.jcim.1c01334},
  number   = {15},
  journal  = {Journal of Chemical Information and Modeling},
  author   = {García-Ortegón, Miguel and Simm, Gregor N. C. and Tripp, Austin J. and Hernández-Lobato, José Miguel and Bender, Andreas and Bacallado, Sergio},
  year     = {2022},
  month    = aug,
  pages    = {3486–3502},
  language = {en}
}

@MISC{Al-Roomi:continuous_objective_benchmarks:2015,
author = {Ali R. Al-Roomi},
title = {{Unconstrained Single-Objective Benchmark Functions Repository}},
year = {2015},
address = {Halifax, Nova Scotia, Canada},
institution = {Dalhousie University, Electrical and Computer Engineering},
url = {https://www.al-roomi.org/benchmarks/unconstrained}
}

@inproceedings{
gao:PMO:2022,
title={Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization},
author={Wenhao Gao and Tianfan Fu and Jimeng Sun and Connor W. Coley},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=yCZRdI0Y7G}
}

 @article{Deb:NSGA-2:2002, title={A fast and elitist multiobjective genetic algorithm: NSGA-II}, volume={6}, ISSN={1089778X}, DOI={10.1109/4235.996017}, number={2}, journal={IEEE Transactions on Evolutionary Computation}, author={Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.}, year={2002}, month=apr, pages={182–197}, language={en} }

@ARTICLE{pymoo,
    author={J. {Blank} and K. {Deb}},
    journal={IEEE Access},
    title={pymoo: Multi-Objective Optimization in Python},
    year={2020},
    volume={8},
    number={},
    pages={89497-89509},
}

 @article{Shahriari:BOReview:2016, title={Taking the Human Out of the Loop: A Review of Bayesian Optimization}, volume={104}, ISSN={0018-9219, 1558-2256}, DOI={10.1109/JPROC.2015.2494218}, abstractNote={Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable conﬁguration parameters. These parameters are often speciﬁed and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in signiﬁcant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.}, number={1}, journal={Proceedings of the IEEE}, author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and De Freitas, Nando}, year={2016}, month=jan, pages={148–175}, language={en} }

 @inproceedings{Kirschner:LineBO:2019, title={Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces}, ISSN={2640-3498}, url={https://proceedings.mlr.press/v97/kirschner19a.html}, abstractNote={Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems that can be solved efficiently. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm. When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings. We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance. Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.}, booktitle={Proceedings of the 36th International Conference on Machine Learning}, publisher={PMLR}, author={Kirschner, Johannes and Mutny, Mojmir and Hiller, Nicole and Ischebeck, Rasmus and Krause, Andreas}, year={2019}, month=may, pages={3429–3438}, language={en} }


 @article{Balandat:botorch:2020, title={BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization}, url={http://arxiv.org/abs/1910.06403}, abstractNote={Bayesian optimization provides sample-efficient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce BoTorch, a modern programming framework for Bayesian optimization that combines Monte-Carlo (MC) acquisition functions, a novel sample average approximation optimization approach, auto-differentiation, and variance reduction techniques. BoTorch’s modular design facilitates flexible specification and optimization of probabilistic models written in PyTorch, simplifying implementation of new acquisition functions. Our approach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel “one-shot” formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efficiency of BoTorch relative to other popular libraries.}, note={arXiv:1910.06403 [cs, math, stat]}, number={arXiv:1910.06403}, publisher={arXiv}, journal={arXiv}, author={Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan}, year={2020}, month=dec }

@inproceedings{gardner:gpytorch:2018,
  title={GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
  author={Gardner, Jacob R and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

 @article{GomezBombarelli:VAEsAndOpt:2018, title={Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules}, volume={4}, ISSN={2374-7943}, DOI={10.1021/acscentsci.7b00572}, abstractNote={We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations of molecules allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in a set of molecules with fewer that nine heavy atoms.}, number={2}, journal={ACS Central Science}, publisher={American Chemical Society}, author={Gómez-Bombarelli, Rafael and Wei, Jennifer N. and Duvenaud, David and Hernández-Lobato, José Miguel and Sánchez-Lengeling, Benjamín and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and Aspuru-Guzik, Alán}, year={2018}, month=feb, pages={268–276} }

@INPROCEEDINGS{Hansen:CMA-ES:1996,
  author={Hansen, N. and Ostermeier, A.},
  booktitle={Proceedings of IEEE International Conference on Evolutionary Computation}, 
  title={Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation}, 
  year={1996},
  volume={},
  number={},
  pages={312-317},
  doi={10.1109/ICEC.1996.542381}}


@InProceedings{jain:multiobjective-gflownet:2023,
  title = 	 {Multi-Objective {GF}low{N}ets},
  author =       {Jain, Moksh and Raparthy, Sharath Chandra and Hern\'{a}ndez-Garc\'{\i}a, Alex and Rector-Brooks, Jarrid and Bengio, Yoshua and Miret, Santiago and Bengio, Emmanuel},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {14631--14653},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/jain23a/jain23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/jain23a.html},
  abstract = 	 {We study the problem of generating <em>diverse</em> candidates in the context of Multi-Objective Optimization. In many applications of machine learning such as drug discovery and material design, the goal is to generate candidates which simultaneously optimize a set of potentially conflicting objectives. Moreover, these objectives are often imperfect evaluations of some underlying property of interest, making it important to generate diverse candidates to have multiple options for expensive downstream evaluations. We propose Multi-Objective GFlowNets (MOGFNs), a novel method for generating diverse Pareto optimal solutions, based on GFlowNets. We introduce two variants of MOGFNs: MOGFN-PC, which models a family of independent sub-problems defined by a scalarization function, with reward-conditional GFlowNets, and MOGFN-AL, which solves a sequence of sub-problems defined by an acquisition function in an active learning loop. Our experiments on wide variety of synthetic and benchmark tasks demonstrate advantages of the proposed methods in terms of the Pareto performance and importantly, improved candidate diversity, which is the main contribution of this work.}
}

